{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMEh7E6MxHRVeyBFJc43ANe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doondu/pytorchTutorial/blob/main/lab06_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sigmoid 함수는 입력값이 어떤 범위에 있든 항상 0과 1 사이의 값을 반환합니다.\n",
        "Softmax 함수는 입력 벡터를 다중 클래스 분류 문제에서 각 클래스에 속할 확률로 변환하기 위해 사용됩니다. 출력 값들은 0과 1 사이의 값이 되며, 모든 출력 값의 합은 1이 됩니다."
      ],
      "metadata": {
        "id": "wv40SWY42nuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW2v3lYezkqc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(777)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5],\n",
        "          [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]] #(8, 4)\n",
        "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],\n",
        "          [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]] #(8, 3)\n",
        "\n",
        "X = Variable(torch.Tensor(x_data))\n",
        "Y = Variable(torch.Tensor(y_data))\n",
        "\n",
        "nb_classes = 3"
      ],
      "metadata": {
        "id": "7eK0dG3l2_14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = torch.nn.Softmax()\n",
        "linear = torch.nn.Linear(4, nb_classes, bias=True) #in_feature = 입력 특성의 크기 4, out_feature 3\n",
        "model = torch.nn.Sequential(linear, softmax)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) #gradient, 미분"
      ],
      "metadata": {
        "id": "jZ96kvNe3grG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(2001):\n",
        "    optimizer.zero_grad() #초기화\n",
        "    hypothesis = model(X) #예측값\n",
        "    cost = -Y * torch.log(hypothesis) #cost = -(Y * torch.log(hypothesis) + (1 - Y)* torch.log(1 - hypothesis)).mean() 랑 같은거래,\n",
        "    #왜냐면 Y 값이 0 또는 1밖에 없기 때문, ONE-HOT encoding\n",
        "    cost = torch.sum(cost, 1).mean() # loss\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 200 == 0:\n",
        "        print(step, cost.data.numpy())\n"
      ],
      "metadata": {
        "id": "FY4OaTOE4Xbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#새 input 값 주고 측정\n",
        "print('[1]-----------------------------')\n",
        "a = model(Variable(torch.Tensor([[1, 11, 7, 9]])))\n",
        "print(a.data.numpy(), torch.max(a, 1)[1].data.numpy()) # index 1 값이 가장 높음\n",
        "\n",
        "print('[2]-----------------------------')\n",
        "b = model(Variable(torch.Tensor([[1, 3, 4, 3]])))\n",
        "print(b.data.numpy(), torch.max(b, 1)[1].data.numpy())\n",
        "\n",
        "print('[3]-----------------------------')\n",
        "c = model(Variable(torch.Tensor([[1, 1, 0, 1]])))\n",
        "print(c.data.numpy(), torch.max(c, 1)[1].data.numpy())\n",
        "\n",
        "print('[4]-----------------------------')\n",
        "all = model(Variable(torch.Tensor([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]])))\n",
        "print(all.data.numpy(), torch.max(all, 1)[1].data.numpy())"
      ],
      "metadata": {
        "id": "FPUAtvBeCpPF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}